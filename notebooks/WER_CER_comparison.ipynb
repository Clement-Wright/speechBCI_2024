{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline vs. Enhanced Decoder Metrics\n",
    "\n",
    "This notebook visualises word error rate (WER) and character accuracy for multiple decoding pipelines. Populate the metric files from `eval_competition.py` or your Weights & Biases exports, then re-run the cells below to refresh the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected inputs\n",
    "\n",
    "* `metrics/baseline_metrics.json` \u2014 dictionary with keys `wer` and `char_accuracy`.\n",
    "* `metrics/enhanced_metrics.json` \u2014 same structure for the LM-augmented or curriculum-trained model.\n",
    "\n",
    "If the files are missing the notebook will fall back to illustrative values so the plotting code still runs. Update `METRIC_SPECS` if you log additional systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(path: Path, fallback: Dict[str, float]) -> Dict[str, float]:\n",
    "    if path.exists():\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "            data = json.load(handle)\n",
    "        return {\n",
    "            \"wer\": float(data.get(\"wer\", fallback[\"wer\"])),\n",
    "            \"char_accuracy\": float(data.get(\"char_accuracy\", fallback[\"char_accuracy\"])),\n",
    "        }\n",
    "    return fallback\n",
    "\n",
    "METRIC_SPECS = {\n",
    "    \"Baseline\": (Path(\"metrics/baseline_metrics.json\"), {\"wer\": 0.285, \"char_accuracy\": 0.86}),\n",
    "    \"Enhanced (Curriculum + LM)\": (Path(\"metrics/enhanced_metrics.json\"), {\"wer\": 0.192, \"char_accuracy\": 0.91}),\n",
    "}\n",
    "\n",
    "records = []\n",
    "for label, (metric_path, fallback) in METRIC_SPECS.items():\n",
    "    metrics = load_metrics(metric_path, fallback)\n",
    "    metrics[\"system\"] = label\n",
    "    records.append(metrics)\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "\n",
    "wer_bars = ax1.bar(x - width/2, df[\"wer\"], width, label=\"WER\", color=\"tab:red\")\n",
    "ax1.set_ylabel(\"Word Error Rate\")\n",
    "ax1.set_ylim(0, max(df[\"wer\"]) * 1.2)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "acc_bars = ax2.bar(x + width/2, df[\"char_accuracy\"], width, label=\"Char Accuracy\", color=\"tab:blue\")\n",
    "ax2.set_ylabel(\"Character Accuracy\")\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df[\"system\"], rotation=15, ha=\"right\")\n",
    "ax1.set_title(\"Decoder Performance Comparison\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.legend(loc=\"upper center\", ncol=2, bbox_to_anchor=(0.5, 1.08))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}